{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee157dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask in /users/xiet13/.local/lib/python3.11/site-packages (2023.10.1)\n",
      "Requirement already satisfied: click>=8.0 in /users/xiet13/.local/lib/python3.11/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /users/xiet13/.local/lib/python3.11/site-packages (from dask) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /users/xiet13/.local/lib/python3.11/site-packages (from dask) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from dask) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /users/xiet13/.local/lib/python3.11/site-packages (from dask) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /users/xiet13/.local/lib/python3.11/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from dask) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask) (3.11.0)\n",
      "Requirement already satisfied: locket in /users/xiet13/.local/lib/python3.11/site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /users/xiet13/.local/lib/python3.11/site-packages (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /users/xiet13/.local/lib/python3.11/site-packages (from pyarrow) (1.26.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: recordlinkage in /users/xiet13/.local/lib/python3.11/site-packages (0.16)\n",
      "Requirement already satisfied: jellyfish>=1 in /users/xiet13/.local/lib/python3.11/site-packages (from recordlinkage) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /users/xiet13/.local/lib/python3.11/site-packages (from recordlinkage) (1.26.1)\n",
      "Requirement already satisfied: pandas<3,>=1 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from recordlinkage) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1 in /users/xiet13/.local/lib/python3.11/site-packages (from recordlinkage) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1 in /users/xiet13/.local/lib/python3.11/site-packages (from recordlinkage) (1.3.2)\n",
      "Requirement already satisfied: joblib in /users/xiet13/.local/lib/python3.11/site-packages (from recordlinkage) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /users/xiet13/.local/lib/python3.11/site-packages (from scikit-learn>=1->recordlinkage) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\n",
      "Current RAM usage: 302.41 MB\n",
      "MemTotal:       258379312 kB\n",
      "MemFree:        255015456 kB\n",
      "MemAvailable:   255303108 kB\n",
      "Buffers:            2704 kB\n",
      "Cached:          2152408 kB\n",
      "SwapCached:            0 kB\n",
      "Active:           501176 kB\n",
      "Inactive:        2418840 kB\n",
      "Active(anon):        608 kB\n",
      "Inactive(anon):   765188 kB\n",
      "Active(file):     500568 kB\n",
      "Inactive(file):  1653652 kB\n",
      "Unevictable:           0 kB\n",
      "Mlocked:               0 kB\n",
      "SwapTotal:             0 kB\n",
      "SwapFree:              0 kB\n",
      "Dirty:                16 kB\n",
      "Writeback:             0 kB\n",
      "AnonPages:        765084 kB\n",
      "Mapped:           299792 kB\n",
      "Shmem:               892 kB\n",
      "KReclaimable:     113528 kB\n",
      "Slab:             227592 kB\n",
      "SReclaimable:     113528 kB\n",
      "SUnreclaim:       114064 kB\n",
      "KernelStack:       11456 kB\n",
      "PageTables:        12624 kB\n",
      "NFS_Unstable:          0 kB\n",
      "Bounce:                0 kB\n",
      "WritebackTmp:          0 kB\n",
      "CommitLimit:    129189656 kB\n",
      "Committed_AS:    8270804 kB\n",
      "VmallocTotal:   34359738367 kB\n",
      "VmallocUsed:       21004 kB\n",
      "VmallocChunk:          0 kB\n",
      "Percpu:            17664 kB\n",
      "HardwareCorrupted:     0 kB\n",
      "AnonHugePages:         0 kB\n",
      "ShmemHugePages:        0 kB\n",
      "ShmemPmdMapped:        0 kB\n",
      "FileHugePages:         0 kB\n",
      "FilePmdMapped:         0 kB\n",
      "HugePages_Total:       0\n",
      "HugePages_Free:        0\n",
      "HugePages_Rsvd:        0\n",
      "HugePages_Surp:        0\n",
      "Hugepagesize:       2048 kB\n",
      "Hugetlb:               0 kB\n",
      "DirectMap4k:      212892 kB\n",
      "DirectMap2M:     5695488 kB\n",
      "DirectMap1G:    256901120 kB\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas\n",
    "!pip install dask\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet phonetics fuzzywuzzy jellyfish putils -q\n",
    "!pip install seaborn -q \n",
    "!pip install matplotlib -q \n",
    "\n",
    "import pandas as pd\n",
    "!pip install recordlinkage\n",
    "import recordlinkage as rl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import psutil\n",
    "import psutil\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.index import SortedNeighbourhood\n",
    "import pickle\n",
    "\n",
    "def get_matches(df, cols):\n",
    "  df = df.reset_index()\n",
    "  df = merge_tid_lmk(df)\n",
    "  for col in cols:\n",
    "    df = df[df[col] != 0]\n",
    "  return df\n",
    "\n",
    "# Function to report RAM usage\n",
    "def report_ram_usage():\n",
    "    process = psutil.Process()\n",
    "    ram_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
    "    print(f\"Current RAM usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "# Report RAM before starting\n",
    "report_ram_usage()\n",
    "\n",
    "# get cpu number\n",
    "import multiprocessing\n",
    "multiprocessing.cpu_count()\n",
    "\n",
    "# get RAM\n",
    "!cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034b452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b51fdb-86c1-450c-b167-8f3608c29e3a",
   "metadata": {},
   "source": [
    "### For loop to get all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9279774-32bf-46c8-b01a-2a611efe9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['11-01', '01-91', '91-81', '81-61', '61-51']\n",
    "years = ['1851', '1861', '1881', '1891', '1901', '1911']\n",
    "toys_with_features = {}\n",
    "candidates_all_year = {}\n",
    "for name in names:\n",
    "    # reading candidates\n",
    "    filename = f'../../output/task1/{name}_candidates.sav'\n",
    "    candidates_all_year[name] = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "# reading toy datasets with features\n",
    "# years = ['1851', '1861', '1881', '1891', '1901', '1911']\n",
    "for year in years:\n",
    "    df = pd.read_parquet(f'../../data_toy/task1/{year}_anynomised.parquet').rename(columns = {'ConParID_18511911':'parID', 'CONPARID_birth':'birth_parID', 'sex':'sex'}).set_index('recid')\n",
    "    toys_with_features[year] = df\n",
    "# test consistency\n",
    "assert toys_with_features['1911'].index.intersection(candidates_all_year['11-01'].get_level_values(0)).shape[0] == toys_with_features['1911'].shape[0]\n",
    "assert toys_with_features['1901'].index.intersection(candidates_all_year['11-01'].get_level_values(1)).shape[0] == toys_with_features['1901'].shape[0]\n",
    "assert toys_with_features['1851'].index.intersection(candidates_all_year['61-51'].get_level_values(1)).shape[0] == toys_with_features['1851'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9c0cda-30b7-41c4-badf-e54233c09949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pname', 'oname', 'sname', 'pname_soundex', 'sname_soundex',\n",
       "       'pname_metaphone', 'sname_metaphone', 'address', 'sname_pop_metaphone',\n",
       "       'dateofbirth', 'sex', 'birth_parID', 'parID', 'pname_mom_soundex',\n",
       "       'sname_mom_soundex', 'pname_pop_soundex', 'sname_pop_soundex',\n",
       "       'pname_sp_soundex', 'sname_sp_soundex', 'pname_mom_metaphone',\n",
       "       'sname_mom_metaphone', 'pname_pop_metaphone', 'pname_sp_metaphone',\n",
       "       'sname_sp_metaphone', '_51'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toys_with_features['1851'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0813452-34b8-4b3e-8d81-707c1115268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing comparer...\n",
      "Current RAM usage: 4667.95 MB\n",
      "CPU times: user 330 ms, sys: 84.4 ms, total: 415 ms\n",
      "Wall time: 538 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import psutil\n",
    "from Utils.sex_1_2_9 import sex_1_2_9\n",
    "from Utils.geo import GeoTable\n",
    "# Initialize the comparer directly in the script\n",
    "print(\"Initializing comparer...\")\n",
    "# comparer = rl.Compare(n_jobs = -1)  # Utilizing all available CPUs\n",
    "comparer = rl.Compare()\n",
    "\n",
    "# cluster_features = ['pname', 'oname', 'sname', 'pname_soundex', 'sname_soundex', 'pname_metaphone', 'sname_metaphone', 'address', 'dateofbirth', 'par_witin_10km', 'birth_par_witin_10km', 'sname_pop_metaphone', 'sex']\n",
    "\n",
    "# Define similarity methods for each attribute pair\n",
    "str_methods = {\n",
    "    'pname': 'jarowinkler',\n",
    "    'oname': 'jarowinkler',\n",
    "    'sname': 'jarowinkler',\n",
    "    'pname_soundex': 'jarowinkler',\n",
    "    'sname_soundex': 'jarowinkler',\n",
    "    'pname_metaphone': 'jarowinkler',\n",
    "    'sname_metaphone': 'jarowinkler',\n",
    "    'address': 'levenshtein',\n",
    "    # new ones in the 1st iteration\n",
    "    'sname_pop_metaphone': 'jarowinkler'\n",
    "}\n",
    "\n",
    "# Add string comparison methods to comparer\n",
    "for attr, method in str_methods.items():\n",
    "    comparer.string(attr, attr, method=method)\n",
    "\n",
    "# Add numeric comparison for the 'dateofbirth' column\n",
    "comparer.numeric('dateofbirth', 'dateofbirth', method = 'gauss', offset = 0, scale = 5) \n",
    "# sex\n",
    "comparer.add(sex_1_2_9('sex', 'sex', 'sex'))\n",
    "\n",
    "# geo\n",
    "buffer_10k = pd.read_csv('../../data_toy/task1/Buffer_10k.csv')\n",
    "pairs_within_10km = {pair['conparid_1']:pair['conparid_2'] for pair in buffer_10k.to_dict(orient = 'records')}\n",
    "\n",
    "comparer.add(GeoTable('parID', 'parID',  pairs_within_10km, label = 'par_witin_10km'))\n",
    "comparer.add(GeoTable('birth_parID', 'birth_parID', pairs_within_10km, label = 'birth_par_witin_10km'))\n",
    "\n",
    "# Report RAM after adding methods\n",
    "report_ram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236f7727-4c90-42ac-87fd-36021eead40e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages/pandas/io/parquet.py:189: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages/pandas/io/parquet.py:189: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages/pandas/io/parquet.py:189: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/apps/hand/miniconda/23.5.2/lib/python3.11/site-packages/pandas/io/parquet.py:189: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# right = ['1851', '1861', '1881', '1891', '1901']\n",
    "# left = ['1861', '1881', '1891', '1901', '1911']\n",
    "\n",
    "right = ['1861', '1881', '1891', '1901']\n",
    "left = ['1881', '1891', '1901', '1911']\n",
    "\n",
    "cols_to_compare = list(str_methods.keys())\n",
    "# add dateofbirth sex birth_parID parID to cols_to_compare\n",
    "to_add = ['dateofbirth', 'sex', 'birth_parID', 'parID']\n",
    "cols_to_compare.extend(to_add)\n",
    "\n",
    "for l_y, r_y in zip(left, right):\n",
    "    compare_name = l_y[-2:] + '-' + r_y[-2:]\n",
    "    result = comparer.compute(candidates_all_year[compare_name], toys_with_features[l_y], toys_with_features[r_y])\n",
    "    result.to_parquet(f'../../output/task1/{compare_name}_compared.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "917be845-4b16-4b31-88eb-9a4ae04916b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 xiet13 cluster-users 181M Mar  5 12:32 ../../output/task1/01-91_compared.parquet\n",
      "-rw-r--r-- 1 xiet13 cluster-users 181M Mar  5 12:41 ../../output/task1/11-01_compared.parquet\n",
      "-rw-r--r-- 1 xiet13 cluster-users 177M Mar  4 16:10 ../../output/task1/61-51_compared.parquet\n",
      "-rw-r--r-- 1 xiet13 cluster-users 177M Mar  5 12:14 ../../output/task1/81-61_compared.parquet\n",
      "-rw-r--r-- 1 xiet13 cluster-users 181M Mar  5 12:23 ../../output/task1/91-81_compared.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ../../output/task1/**parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b2389-ada5-485d-a6fc-aa53030e76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.birth_par_witin_10km.value_counts(), result.par_witin_10km.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6aa7ee-07a7-461f-85bd-20e2468c7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb5c2f",
   "metadata": {},
   "source": [
    "## Record Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cb3b9-1dec-4cd5-98a2-f468a905b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import LocalCluster \n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# # client = LocalCluster(n_workers = 32)\n",
    "# client = LocalCluster(n_workers = 8, threads_per_worker=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "# import recordlinkage as rl\n",
    "# from recordlinkage.index import SortedNeighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc3b02",
   "metadata": {},
   "source": [
    "### Initialize the indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lha ../Output/temp/{name}/name_soundex_and_district_birth_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92482297",
   "metadata": {},
   "source": [
    "### Read indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # name = \"\"\n",
    "# name = \"91-81\"\n",
    "# read_from_multiple = True\n",
    "\n",
    "# if read_from_multiple:\n",
    "#     # initiate an empty MultiIndex with recid_1 and recid_2 as index\n",
    "#     # candidates = pd.MultiIndex(levels=[[], []], codes=[[], []], names=['recid_1', 'recid_2'])\n",
    "#     candidates = []\n",
    "#     # all files in directory\n",
    "#     import os\n",
    "#     for file in os.listdir(f\"../Output/temp/{name}/name_soundex_and_district_birth_ID\"):\n",
    "#         if file.endswith(\".sav\"):\n",
    "#             candidates.append(pickle.load(open(os.path.join(f\"../Output/temp/{name}/name_soundex_and_district_birth_ID\", file), 'rb')))\n",
    "#             print(f\"loaded {file}\")\n",
    "#     # union is too slow\n",
    "#     # get them rogether\n",
    "#     candidates = pd.concat([c.to_frame().reset_index(drop = True).drop_duplicates(subset = ['recid_1', 'recid_2']) for c in candidates], ignore_index=True)\n",
    "#     candidates = candidates.set_index(['recid_1', 'recid_2']).index\n",
    "# else:\n",
    "#     filename = f'../Output/{name}_pool_idx.sav'\n",
    "#     # pickle.dump(candidates, open(filename, 'wb'))\n",
    "#     candidates = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# # check if candidates is unique\n",
    "# candidates.is_unique # 1m 7.3s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afeef5e",
   "metadata": {},
   "source": [
    "### Reread with comparing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_use = ['recid','pname',\n",
    "#  'oname',\n",
    "#  'sname',\n",
    "#  'pname_soundex',\n",
    "#  'sname_soundex',\n",
    "#  'pname_metaphone',\n",
    "#  'sname_metaphone',\n",
    "#  'address',\n",
    "#  'sname_pop_metaphone',\n",
    "#  'dateofbirth',\n",
    "#  'sex',\n",
    "#  'CONPARID_birth',\n",
    "#  'ConParID_18511911',\n",
    "#  ]\n",
    "\n",
    "# _1881 = dd.read_parquet('../data_toy/task1/1881_anynomised.parquet', columns = to_use).compute(scheduler = 'processes') # unique\n",
    "# _1881 = _1881.drop_duplicates(subset = ['recid'])\n",
    "# _1891 = dd.read_parquet('../data_toy/task1/1881_anynomised.parquet', columns = to_use).compute(scheduler = 'processes') # unique\n",
    "# _1891 = _1891.drop_duplicates(subset = ['recid'])\n",
    "\n",
    "# # renaming\n",
    "# _1881 = _1881.rename(columns = {'ConParID_18511911':'parID', 'CONPARID_birth':'birth_parID', 'sex':'sex'}).set_index('recid')\n",
    "# _1891 = _1891.rename(columns = {'ConParID_18511911':'parID', 'CONPARID_birth':'birth_parID', 'sex':'sex'}).set_index('recid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for pool quality\n",
    "# candidates.get_level_values(0).unique() * candidates.get_level_values(1).unique() / candidates.shape[0] # 242155\n",
    "# 11300570 * 11068409 / 516524465 # 242155\n",
    "\n",
    "# 516524465 / (19828561 * 17711037) # 1.4708070690279348e-06: proportion to full indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b70eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _1891.index.unique() # 19828561\n",
    "# _1851.index.unique() # 17711037\n",
    "# candidates.get_level_values(0).unique() # 11300570 (57.00% of 1861)\n",
    "# candidates.get_level_values(1).unique() # 11068409 (62.49% of 1851)\n",
    "\n",
    "# # subset candidates with get_level_values(0) in 1861.index and get_level_values(1) in 1851.index\n",
    "# candidates_sub = candidates[candidates.get_level_values(0).isin(_1891.index) & candidates.get_level_values(1).isin(_1851.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if candidates is unique\n",
    "# candidates.is_unique\n",
    "# check if all candidates is within 1851 and 1861\n",
    "assert all(candidates.get_level_values(0).isin(_1891.index) & candidates.get_level_values(1).isin(_1891.index))\n",
    "\n",
    "ddf__1881 = _1881\n",
    "ddf__1891 = _1891"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a89c6e",
   "metadata": {},
   "source": [
    "### Comparer Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a90b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import psutil\n",
    "# from Utils.sex_1_2_9 import sex_1_2_9\n",
    "# from Utils.geo import GeoTable\n",
    "# # Initialize the comparer directly in the script\n",
    "# print(\"Initializing comparer...\")\n",
    "# # comparer = rl.Compare(n_jobs = -1)  # Utilizing all available CPUs\n",
    "# comparer = rl.Compare()\n",
    "\n",
    "# # cluster_features = ['pname', 'oname', 'sname', 'pname_soundex', 'sname_soundex', 'pname_metaphone', 'sname_metaphone', 'address', 'dateofbirth', 'par_witin_10km', 'birth_par_witin_10km', 'sname_pop_metaphone', 'sex']\n",
    "\n",
    "# # Define similarity methods for each attribute pair\n",
    "# str_methods = {\n",
    "#     'pname': 'jarowinkler',\n",
    "#     'oname': 'jarowinkler',\n",
    "#     'sname': 'jarowinkler',\n",
    "#     'pname_soundex': 'jarowinkler',\n",
    "#     'sname_soundex': 'jarowinkler',\n",
    "#     'pname_metaphone': 'jarowinkler',\n",
    "#     'sname_metaphone': 'jarowinkler',\n",
    "#     'address': 'levenshtein',\n",
    "#     # new ones in the 1st iteration\n",
    "#     'sname_pop_metaphone': 'jarowinkler'\n",
    "# }\n",
    "\n",
    "# # Add string comparison methods to comparer\n",
    "# for attr, method in str_methods.items():\n",
    "#     comparer.string(attr, attr, method=method)\n",
    "\n",
    "\n",
    "# # Define ID columns for exact comparison\n",
    "# # ID_cols = [\n",
    "# #     'subdist_ID', 'district_ID', 'county_ID', 'CONPARID_birth',\n",
    "# #     'subdist_birth_ID', 'district_birth_ID', 'county_birth_ID',\n",
    "# #     'recid_mom', 'recid_pop', 'recid_sp'\n",
    "# # ]\n",
    "# # ID_cols = [\n",
    "# #     'recid_mom', 'recid_pop', 'recid_sp' # make features for families, or use later \n",
    "# # ]\n",
    "# # # Add exact comparison methods to comparer for ID columns\n",
    "# # for attr in ID_cols:\n",
    "# #     comparer.exact(attr, attr)\n",
    "\n",
    "\n",
    "# # Add numeric comparison for the 'dateofbirth' column\n",
    "# comparer.numeric('dateofbirth', 'dateofbirth', method = 'gauss', offset = 0, scale = 5) \n",
    "# # sex\n",
    "# comparer.add(sex_1_2_9('sex', 'sex', 'sex'))\n",
    "\n",
    "# # geo\n",
    "# buffer_10k = pd.read_csv('../Census_samples/Buffer_10k.csv')\n",
    "# pairs_within_10km = {pair['conparid_1']:pair['conparid_2'] for pair in buffer_10k.to_dict(orient = 'records')}\n",
    "\n",
    "# comparer.add(GeoTable('parID', 'parID',  pairs_within_10km, label = 'par_witin_10km'))\n",
    "# comparer.add(GeoTable('birth_parID', 'birth_parID', pairs_within_10km, label = 'birth_par_witin_10km'))\n",
    "# # comparer.add(GeoTable('parID', 'parID', pairs_within_10km,  'par_witin_10km'))\n",
    "# # comparer.add(GeoTable('birth_parID', 'birth_parID', pairs_within_10km, 'birth_par_witin_10km'))\n",
    "\n",
    "# # comparer.numeric('sex', 'sex', method = 'lin', offset = 0, scale = 5)  \n",
    "# # # for {0:m, 5:unknown, 10:female} sex value design: (0,0) get 1, (0,5) get 0.5, (0,10) get 0. \n",
    "# # # Imprecise account for (5,5) case, where should be 0.5, but get 1.\n",
    "\n",
    "# # https://www.elastic.co/guide/en/elasticsearch/guide/current/decay-functions.html\n",
    "\n",
    "# # Custom comparer: Add checking if CONPARIDs are within 10 km\n",
    "# # comparer.add(WithinDist('ConParID_18511911', 'ConParID_18511911', pairs_within_10km))\n",
    "# # comparer.add(WithinDist('CONPARID_birth', 'CONPARID_birth', pairs_within_10km))\n",
    "    \n",
    "\n",
    "# # Report RAM after adding methods\n",
    "# report_ram_usage()\n",
    "# _1881"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee06abf",
   "metadata": {},
   "source": [
    "### Perform Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca09c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_compare = list(str_methods.keys())\n",
    "# # add dateofbirth sex birth_parID parID to cols_to_compare\n",
    "# to_add = ['dateofbirth', 'sex', 'birth_parID', 'parID']\n",
    "# cols_to_compare.extend(to_add)\n",
    "# result = comparer.compute(candidates, df1, df2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fe9c6",
   "metadata": {},
   "source": [
    "#### Cluster for comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfca9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import LocalCluster, Client\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# # client = LocalCluster(n_workers = 32)\n",
    "# # client = LocalCluster(n_workers = 4, threads_per_worker = 8,  interface='lo')\n",
    "# clinet = Client(n_workers = 4, threads_per_worker = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import dask\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "# # Perform the actual comparisons and store the result\n",
    "# print(\"Performing comparisons...\")\n",
    "\n",
    "# def compute_and_save_partition(comparer, candidates, df1, df2, partition_path):\n",
    "#     if os.path.exists(partition_path):\n",
    "#         print(f\"Skipping already processed partition: {partition_path}\")\n",
    "#         return\n",
    "#     # Perform the comparison\n",
    "#     print(f\"Computing partition: {partition_path}\")\n",
    "#     result = comparer.compute(candidates, df1, df2, n_jobs=-1)\n",
    "#     # Save the result\n",
    "#     result.to_parquet(partition_path)\n",
    "#     print(f\"Saved {partition_path}\")\n",
    "#     # Report RAM usage\n",
    "#     report_ram_usage()\n",
    "#     # Return the result\n",
    "#     return result\n",
    "\n",
    "\n",
    "# # def parallel_compute_and_save(comparer, candidates, df1, df2, output_dir, partition=1000000):\n",
    "# #     tasks = []\n",
    "# #     total_candidates = candidates.shape[0]\n",
    "# #     for i in range(0, total_candidates, partition):\n",
    "# #         end = min(i + partition, total_candidates)\n",
    "# #         partition_path = \n",
    "# #         print(f\"Sliced {end} - {i}\\n\")\n",
    "# #         tasks.append(compute_and_save_partition(comparer, candidates[start:end].drop_duplicates(), df1, df2, partition_path))\n",
    "# #         print(f\"Finished {i} - {end}\\n\")\n",
    "# #     return tasks\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# output_dir = f\"../Output/temp/{name}_compare_corrected_13/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# partition=1000000\n",
    "\n",
    "# # Applying the delayed function with saving and loading\n",
    "# tasks = [dask.delayed(compute_and_save_partition)(comparer, candidates[i:i+partition], ddf__1891[cols_to_compare], ddf__1881[cols_to_compare], \n",
    "#             os.path.join(output_dir, f'partition_{i}_{i+partition}.parquet'))\n",
    "#             for i in range(0, candidates.shape[0], partition)]\n",
    "\n",
    "# # Compute the result\n",
    "# with dask.diagnostics.ProgressBar():\n",
    "#     dask.compute(*tasks)\n",
    "\n",
    "# # Compute all tasks in parallel\n",
    "# # dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844adab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aa\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import dask\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "# # Perform the actual comparisons and store the result\n",
    "# print(\"Performing comparisons...\")\n",
    "\n",
    "# # @dask.delayed\n",
    "# def compute_and_save_partition(comparer, candidates, df1, df2, start, end, partition_path):\n",
    "#     # Check if the partition is already computed and saved\n",
    "#     try:\n",
    "#         # Check if the partition is already computed and saved\n",
    "#         if os.path.exists(partition_path):\n",
    "#             # Load the partition from the Parquet file\n",
    "#             return pd.read_parquet(partition_path)\n",
    "#         else:\n",
    "#             # Compute the partition\n",
    "#             candidate_slice = candidates[start:end].drop_duplicates() # len(candidates) == 1000,000\n",
    "#             # if len(candidate_slice) != 1000000:\n",
    "#             #     raise ValueError(f\"candidate_slice has incorrect number of rows: {len(candidate_slice)}\")\n",
    "#             # if len(df1) != 1000000 or len(df2) != 1000000:\n",
    "#             #     raise ValueError(f\"Mismatch in number of rows. df1: {len(df1)}, df2: {len(df2)}\")\n",
    "\n",
    "#             # partition_result = comparer.compute(candidate_slice, df1, df2, n_jobs=-1)\n",
    "#             partition_result = comparer.compute(candidate_slice, df1, df2)\n",
    "#             # Error details: Length mismatch: Expected axis has 1000012 elements, new values have 1000000 elements\n",
    "\n",
    "#             # Save the computed partition to a Parquet file\n",
    "#             partition_result.to_parquet(partition_path)\n",
    "#             return partition_result\n",
    "#     except ValueError as e:\n",
    "#         # Log detailed error information\n",
    "#         print(f\"Error processing partition {partition_path.split('_')[1:-1]}: start={start}, end={end}, candidate_slice: {len(candidate_slice)}\")\n",
    "#         print(f\"Error details: {e}\")\n",
    "#         raise\n",
    "\n",
    "# def parallel_compute_and_save(comparer, candidates, df1, df2, output_dir, partition=1000000):\n",
    "#     result = []\n",
    "#     total_candidates = candidates.shape[0]\n",
    "#     for i in range(0, total_candidates, partition):\n",
    "#         end = min(i + partition, total_candidates)\n",
    "#         partition_path = os.path.join(output_dir, f'partition_{i}_{end}.parquet')\n",
    "#         print(f\"Sliced {end} - {i}\\n\")\n",
    "#         result.append(compute_and_save_partition(comparer, candidates, df1, df2, i, end, partition_path))\n",
    "#     return result\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# output_dir = f\"../Output/temp/{name}_compare_corrected_13/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # # Applying the delayed function with saving and loading\n",
    "\n",
    "# # # Compute the result\n",
    "# # with dask.diagnostics.ProgressBar():\n",
    "# #     result = delayed_result.compute()\n",
    "\n",
    "# with dask.diagnostics.ProgressBar():\n",
    "#     result = parallel_compute_and_save(comparer, candidates, _1891[cols_to_compare], _1881[cols_to_compare], output_dir)\n",
    "\n",
    "# final_result = pd.concat(result)\n",
    "# # Update the columns of the result dataframe\n",
    "# final_result.columns = ['pname', 'oname', 'sname', 'pname_soundex','sname_soundex',\n",
    "#     'pname_metaphone', 'sname_metaphone', 'address','sname_pop_metaphone', 'dateofbirth', 'sex', 'par_witin_10km', 'birth_par_witin_10km']\n",
    "# final_result = final_result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = dd.read_parquet(output_dir)\n",
    "final_result.columns = ['pname', 'oname', 'sname', 'pname_soundex','sname_soundex',\n",
    "    'pname_metaphone', 'sname_metaphone', 'address','sname_pop_metaphone', 'dateofbirth', 'sex', 'par_witin_10km', 'birth_par_witin_10km']\n",
    "final_result = final_result.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a085bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique recid_1\n",
    "len(candidates.get_level_values(0).unique()), len(candidates.get_level_values(1).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lha ../Output/temp/{name}_compare_corrected_13/\n",
    "# count the number of files in the directory\n",
    "!ls -l ../Output/temp/{name}_compare_corrected_13/ | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
