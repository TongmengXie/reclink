{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current RAM usage: 0.35 GiB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import recordlinkage as rl\n",
    "except ImportError:\n",
    "    !pip install recordlinkage\n",
    "    import recordlinkage as rl\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import pyarrow\n",
    "except ImportError:\n",
    "    !pip install pyarrow\n",
    "    import pyarrow\n",
    "\n",
    "# for model zoo\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "except ImportError:\n",
    "    !pip install catboost\n",
    "    import catboost as cb\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    !pip install lightgbm\n",
    "    from lightgbm import LGBMClassifier\n",
    "\n",
    "try:\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "except ImportError:\n",
    "    !pip install scikit-learn\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    !pip install xgboost\n",
    "    import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import pydot\n",
    "    import graphviz\n",
    "except ImportError:\n",
    "    !pip install pydot graphviz\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import pydot\n",
    "    import graphviz\n",
    "import pickle\n",
    "import os\n",
    "# from dask_ml.wrappers import ParallelPostFit\n",
    "# Note: The installation commands (!pip install) will not directly run in this Python script.\n",
    "# They are illustrative. Run these commands in your Jupyter notebook or terminal before executing this script.\n",
    "\n",
    "\n",
    "# get CPU count\n",
    "import multiprocessing\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "\n",
    "def attach_ori_feature(to_attach, to_attach_recid_left, to_attach_recid_right, left_df, right_df, suffixes):\n",
    "    to_attach = to_attach.merge(left_df, left_on = to_attach_recid_left, right_on = 'recid', how = 'left')\n",
    "    to_attach = to_attach.merge(right_df, left_on = to_attach_recid_right, right_on = 'recid', how = 'left', suffixes = suffixes)\n",
    "    return to_attach\n",
    "\n",
    "def plot_feature_importance(fields, feature_importances):\n",
    "    df = pd.DataFrame({\"Feature\": fields, \"Importance\": feature_importances})\n",
    "    df = df.sort_values(\"Importance\", ascending=False)\n",
    "    ax = df.plot(kind='bar', x='Feature', y='Importance', legend=None)\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "import psutil\n",
    "# Function to report RAM usage\n",
    "def report_ram_usage():\n",
    "    process = psutil.Process()\n",
    "    ram_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
    "    # print(f\"Current RAM usage: {ram_usage:.2f} MB\")\n",
    "    # human readable GiB\n",
    "    print(f\"Current RAM usage: {ram_usage / 1024:.2f} GiB\")\n",
    "\n",
    "# Report RAM before starting\n",
    "report_ram_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_THRESHOLD = 0\n",
    "# !pip install -q torch torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MyNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom neural network model.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The size of the input features.\n",
    "        hidden_size (list): A list of integers representing the sizes of the hidden layers.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.Sequential): The sequential layers of the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=[128, 64, 48]):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size[0]), # input layer\n",
    "            nn.BatchNorm1d(hidden_size[0]), # batch normalization\n",
    "            nn.ReLU(), # activation function\n",
    "\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.BatchNorm1d(hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.BatchNorm1d(hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(hidden_size[2], 1),\n",
    "            nn.Sigmoid() # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import LocalCluster \n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# client = LocalCluster(n_workers = 8, threads_per_worker = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['11-01', '01-91', '91-81', '81-61', '61-51']\n",
    "\n",
    "name = \"61-51\"\n",
    "ori_features = ['pname', 'oname', 'sname', 'pname_soundex', 'sname_soundex', 'pname_metaphone', 'sname_metaphone', 'address', 'dateofbirth', 'par_witin_10km', 'birth_par_witin_10km', 'sname_pop_metaphone', 'sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current RAM usage: 16.08 GiB\n"
     ]
    }
   ],
   "source": [
    "matching_pool = pd.read_parquet(f\"../../output/task1/{name}_compared.parquet\") #1m15s\n",
    "# matching_pool = dd.read_parquet(f\"../Output/temp/{name}_compare_corrected_13/\").repartition(npartitions = CPU_COUNT)\n",
    "matching_pool.columns = ['pname', 'oname', 'sname', 'pname_soundex','sname_soundex',\n",
    "    'pname_metaphone', 'sname_metaphone', 'address','sname_pop_metaphone', 'dateofbirth', 'sex', 'par_witin_10km', 'birth_par_witin_10km']\n",
    "matching_pool = matching_pool.reset_index()\n",
    "\n",
    "matching_pool.birth_par_witin_10km = matching_pool.birth_par_witin_10km.astype(float)\n",
    "matching_pool.par_witin_10km = matching_pool.par_witin_10km.astype(float)\n",
    "\n",
    "matching_pool['recid_1'] = matching_pool.recid_1.astype('int64')\n",
    "matching_pool['recid_2'] = matching_pool.recid_2.astype('int64')\n",
    "\n",
    "\n",
    "report_ram_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Models pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current RAM usage: 9.84 GiB\n"
     ]
    }
   ],
   "source": [
    "# read back\n",
    "\n",
    "model_dir = f'../../model/simple_bt_61-51_13cols_smoted/' # model was trained using 61-51 hence '61-51' is contained in the model name\n",
    "model_zoo = {}\n",
    "\n",
    "wrap_dask = False\n",
    "# read all models in the model_dir\n",
    "for model in os.listdir(model_dir):\n",
    "    with open(f'{model_dir}{model}', 'rb') as handle:\n",
    "        if wrap_dask:\n",
    "            model_zoo[model.split('.')[0]] = ParallelPostFit(pickle.load(handle))\n",
    "        else:\n",
    "            model_zoo[model.split('.')[0]] = pickle.load(handle)\n",
    "        # predict using dask version sklearn\n",
    "        # model = ParallelPostFit(model)\n",
    "        handle.close()\n",
    "report_ram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.6M\n",
      "drwxr-xr-x 2 xiet13 cluster-users 6.0K Jan  8 16:25 .\n",
      "drwxr-xr-x 5 xiet13 cluster-users 6.0K Mar  5 12:14 ..\n",
      "-rw-r--r-- 1 xiet13 cluster-users 119K Jan  8 16:25 cb.pkl\n",
      "-rw-r--r-- 1 xiet13 cluster-users 1.3K Jan  8 16:25 gnb.pkl\n",
      "-rw-r--r-- 1 xiet13 cluster-users 339K Jan  8 16:25 lgbm.pkl\n",
      "-rw-r--r-- 1 xiet13 cluster-users 3.0M Jan  8 16:25 rf.pkl\n",
      "-rw-r--r-- 1 xiet13 cluster-users 1.1M Jan  8 16:25 xgb.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -lha ../../model/simple_bt_61-51_13cols_smoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.516199588775635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sample_200 = matching_pool.head(10)\n",
    "val = sys.getsizeof(sample_200) * 500000000 / 10\n",
    "val / 1024**3 # 62.584877014160156\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "par_witin_10km\n",
       "0.0    39313316\n",
       "1.0      355988\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_pool.birth_par_witin_10km.value_counts()\n",
    "# birth_par_witin_10km\n",
    "# 1.0    176835048\n",
    "# 0.0     75023414\n",
    "# Name: count, dtype: int64\n",
    "\n",
    "matching_pool.par_witin_10km.value_counts()\n",
    "# par_witin_10km\n",
    "# 0.0    192624960\n",
    "# 1.0     59233502\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "for model_name, model in model_zoo.items():\n",
    "    matching_pool[f'{model_name}_proba'] = model.predict_proba(matching_pool[ori_features])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching_pool[[f'{model_name}_proba' for model_name in model_zoo.keys()]].to_parquet(f'../../output/task1/{name}_compare_corrected_13_proba.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 xiet13 cluster-users 324M Mar  5 14:00 ../../output/task1/61-51_compare_corrected_13_proba.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lha ../../output/task1/{name}_compare_corrected_13_proba.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def process_chunk(chunk, model_zoo, ori_features, output_path, chunk_index):\n",
    "#     proba = pd.DataFrame(columns=[f'{model_name}_proba' for model_name in model_zoo.keys()], dtype='float64')\n",
    "#     for model_name, model in model_zoo.items():\n",
    "#         proba[f'{model_name}_proba'] = model.predict_proba(chunk[ori_features])[:, 1]\n",
    "#     # Save each chunk to a separate file\n",
    "#     chunk_file = os.path.join(output_path, f\"chunk_{chunk_index}.parquet\")\n",
    "#     if os.path.exists(chunk_file):\n",
    "#         print(f\"Chunk {chunk_index} already exists. Skipping...\")\n",
    "#         pass\n",
    "#     proba.to_parquet(chunk_file)\n",
    "#     print(f\"Saved chunk {chunk_index} to {chunk_file}\")\n",
    "#     report_ram_usage()\n",
    "#     del proba\n",
    "#     del chunk\n",
    "\n",
    "# # Assuming matching_pool is a Dask DataFrame\n",
    "# output_path = f'../Output/temp/{name}_compare_corrected_13_proba'\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "# # chunks = matching_pool.to_delayed()\n",
    "\n",
    "# # make matching pool (pd.df) into 8 chunks\n",
    "# chunks = np.array_split(matching_pool, 8)\n",
    "\n",
    "# # Process each chunk\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     # chunk_df = dd.from_delayed([chunk]).compute()\n",
    "#     process_chunk(chunk, model_zoo, ori_features, output_path, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# model = torch.load(f'../models/simple_nn_{name}_13cols_5ensemble_smoted.pt') \n",
    "model_state_dict = torch.load(f'../../model/simple_nn_61-51_13cols_5ensemble_smoted.pt')\n",
    "nn_features = ['pname',\n",
    " 'oname',\n",
    " 'sname',\n",
    " 'pname_soundex',\n",
    " 'sname_soundex',\n",
    " 'pname_metaphone',\n",
    " 'sname_metaphone',\n",
    " 'address',\n",
    " 'dateofbirth',\n",
    " 'par_witin_10km',\n",
    " 'birth_par_witin_10km',\n",
    " 'sname_pop_metaphone',\n",
    " 'sex',\n",
    " 'gnb_proba',\n",
    " 'cb_proba',\n",
    " 'xgb_proba',\n",
    " 'rf_proba',\n",
    " 'lgbm_proba']\n",
    "nn_model = MyNN(input_size=len(nn_features)) # takes in 18 features:\n",
    "nn_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# one-time inferecne (failed)\n",
    "matching_pool['nn_pred'] = nn_model(torch.tensor(matching_pool[nn_features].values.astype(np.float32))).detach().numpy().squeeze()\n",
    "\n",
    "# output_dir = f'../Output/temp/{name}_compare_corrected_13_nn'\n",
    "# os.makedir(output_dir, exist_ok=True)\n",
    "# matching_pool[['nn_pred']].to_parquet(ps.path.join(output_dir, 'nn_pred.parquet'))\n",
    "\n",
    "\n",
    "# def process_and_save_chunk(df1, df2, model, nn_features, output_dir, chunk_index):\n",
    "#     # Concatenate the partitions\n",
    "#     print(f\"computing {i}\")\n",
    "#     mtpool = dd.concat([df1.reset_index(drop = True), df2.reset_index(drop = True)], axis = 1).compute()\n",
    "#     # mtpool = mtpool.compute()\n",
    "#     # Apply the neural network model\n",
    "#     predictions = model(torch.tensor(mtpool[nn_features].values.astype(np.float32))).detach().numpy().squeeze()\n",
    "#     print(f\"predicted {i}\")\n",
    "#     del mtpool\n",
    "\n",
    "#     # Save the predictions\n",
    "#     # Assuming predictions is a Pandas DataFrame\n",
    "#     save_file = f\"{output_dir}/chunk_{chunk_index}.parquet\"\n",
    "#     if os.path.exists(save_file):\n",
    "#         print(f\"Chunk {chunk_index} already exists. Skipping...\")\n",
    "#         pass\n",
    "#     pd.DataFrame(predictions).to_parquet(save_file, index=False)\n",
    "#     # predictions.to_parquet(f\"{output_dir}/chunk_{chunk_index}.parquet\", index=False)\n",
    "#     print(f\"saved {i}\")\n",
    "#     del predictions\n",
    "#     report_ram_usage()\n",
    "\n",
    "# output_dir = f'../Output/temp/{name}_compare_corrected_13_nn'\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # are matching_pool and proba partitioned the same way?\n",
    "# assert matching_pool.npartitions == proba.npartitions\n",
    "# npartitions = matching_pool.npartitions\n",
    "# for i in range(npartitions):\n",
    "#     process_and_save_chunk(matching_pool.get_partition(i).reset_index(drop = True), proba.get_partition(i).reset_index(drop = True), nn_model, nn_features, output_dir, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92K\n",
      "drwxr-xr-x  5 xiet13 cluster-users 6.0K Mar  5 12:14 .\n",
      "drwxr-xr-x  8 xiet13 cluster-users 6.0K Mar  4 12:46 ..\n",
      "drwxr-xr-x  2 xiet13 cluster-users 6.0K Mar  5 12:10 .ipynb_checkpoints\n",
      "drwxr-xr-x 10 xiet13 cluster-users 6.0K Feb 21 18:10 name_addr_md\n",
      "-rw-r--r--  1 xiet13 cluster-users  902 Mar  5 12:14 readme.md\n",
      "drwxr-xr-x  2 xiet13 cluster-users 6.0K Jan  8 16:25 simple_bt_61-51_13cols_smoted\n",
      "-rw-r--r--  1 xiet13 cluster-users  66K Mar  5 12:08 simple_nn_61-51_13cols_5ensemble_smoted.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lha ../../model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_save_chunk(df1_chunk, df2_chunk, model, nn_features, output_dir, chunk_index):\n",
    "#     # Concatenate the partitions\n",
    "#     mtpool = pd.concat([df1_chunk, df2_chunk], axis=1)\n",
    "\n",
    "#     # Apply the neural network model\n",
    "#     predictions = model(torch.tensor(mtpool[nn_features].values.astype(np.float32))).detach().numpy().squeeze()\n",
    "\n",
    "#     # Save the predictions\n",
    "#     save_file = os.path.join(output_dir, f\"chunk_{chunk_index}.parquet\")\n",
    "#     if not os.path.exists(save_file):\n",
    "#         pd.DataFrame(predictions).to_parquet(save_file, index=False)\n",
    "\n",
    "#     print(f\"Processed chunk {chunk_index}\")\n",
    "\n",
    "\n",
    "# output_dir = f'../Output/temp/{name}_compare_corrected_13_nn'\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# Ensure that partitions of both dataframes are aligned\n",
    "# assert matching_pool.npartitions == proba.npartitions\n",
    "# npartitions = matching_pool.npartitions\n",
    "\n",
    "\n",
    "# for i in range(npartitions):\n",
    "#     # Compute each partition to pandas dataframe\n",
    "#     df1_chunk = matching_pool.get_partition(i).compute().reset_index(drop = True)\n",
    "#     df2_chunk = proba.get_partition(i).compute().reset_index(drop = True).reset_index(drop = True)\n",
    "    \n",
    "#     # Ensure the chunks have the same length\n",
    "#     # min_len = min(len(df1_chunk), len(df2_chunk))\n",
    "#     assert len(df1_chunk) == len(df2_chunk)\n",
    "#     # df1_chunk = df1_chunk.head(min_len)\n",
    "#     # df2_chunk = df2_chunk.head(min_len)\n",
    "\n",
    "#     # Process and save each chunk\n",
    "#     process_and_save_chunk(df1_chunk, df2_chunk, nn_model, nn_features, output_dir, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba = proba.apply(lambda x: x.astype('float16'))\n",
    "# assert matching_pool.index == proba.index\n",
    "# matching_pool = pd.read_parquet(f\"../Output/temp/{name}_compare_corrected_13/\") \n",
    "# matching_pool = pd.concat([matching_pool, proba], axis = 1)\n",
    "# del proba\n",
    "report_ram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matching_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(dataframe, batch_size):\n",
    "    total_size = len(dataframe)\n",
    "    for i in range(0, total_size, batch_size):\n",
    "        yield dataframe.iloc[i:i + batch_size]\n",
    "\n",
    "# Batch size calculation\n",
    "total_rows = matching_pool.shape[0]\n",
    "batches = 8\n",
    "batch_size = total_rows // batches\n",
    "\n",
    "output_dir = f'../Output/temp/{name}_compare_corrected_13_nn'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Create generator\n",
    "data_generator = batch_generator(matching_pool, batch_size)\n",
    "\n",
    "# Process each batch\n",
    "for i, batch in enumerate(data_generator):\n",
    "    batch_data = torch.tensor(batch[nn_features].values.astype(np.float32))\n",
    "    batch_output = nn_model(batch_data).detach().numpy().squeeze()\n",
    "    \n",
    "    # Save the batch outcome\n",
    "    # You can adjust the saving method according to your requirements\n",
    "    output_file = os.path.join(output_dir, f\"batch_output_{i}.npy\")\n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"Saving batch {i+ 1}/{batches + 1} to {output_file}\")\n",
    "        np.save(output_file, batch_output)\n",
    "        print(f'Batch {i+1}/{batches + 1} processed and saved.')\n",
    "    \n",
    "    else:\n",
    "        print(f\"Batch {i+1}/{batches + 1} already exists. Skipping...\")\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read all\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = f'../Output/temp/{name}_compare_corrected_13_nn'\n",
    "try: \n",
    "    nn = np.concatenate([np.load(os.path.join(output_dir, f\"batch_output_{i}.npy\")) for i in range(batches + 1)])\n",
    "except:\n",
    "    nn = np.concatenate([np.load(os.path.join(output_dir, f\"batch_output_{i}.npy\")) for i in range(batches)])\n",
    "len(matching_pool) == len(nn)\n",
    "matching_pool['nn_pred'] = nn\n",
    "report_ram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pool.to_parquet(f'../Output/{name}_compare_corrected_13_nn_proba', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ../Output/{name}_compare_corrected_13_nn_proba/\n",
    "# !rm -rf ../Output/91-81_compare_corrected_13_nn_proba\n",
    "!ls -lha ../Output/{name}_compare_corrected_13_nn_proba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = d.read_parquet(f'../Output/temp/{name}_compare_corrected_13_nn')\n",
    "# temp.columns = ['nn_proba']\n",
    "# temp[temp.nn_proba > 0.5].sum() / temp.shape[0]\n",
    "# # nn_proba    0.39159\n",
    "# # dtype: float64\n",
    "\n",
    "# temp[temp.nn_proba > 0.9999].sum() / temp.shape[0]\n",
    "# # nn_proba    0.016437\n",
    "# # dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_nn = pd.read_parquet(f\"../Output/temp/{name}_compare_corrected_13/\") #1m15s\n",
    "# pred_nn = dd.read_parquet(f\"../Output/temp/{name}_compare_corrected_13_nn/\").compute(schedule = 'processes') \n",
    "# matching_pool[['recid_1', 'recid_2']].compute().reset_index(drop = True).reset_index(drop = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pool = dd.read_parquet(f\"../Output/{name}_compare_corrected_13_nn_proba/\", columns = ['recid_1', 'recid_2', 'nn_pred']).compute(schedule = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pool.recid_1.nunique(), matching_pool.recid_2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantiles\n",
    "matching_pool.nn_pred.quantile([0.1, 0.5, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask \n",
    "mask_true = matching_pool.nn_pred > 0\n",
    "matching_pool['nn_bool']  = False\n",
    "matching_pool.loc[mask_true, 'nn_bool'] = True\n",
    "matching_pool['nn_bool'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pool[mask_true].recid_1.nunique(), matching_pool[mask_true].recid_2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.nn_bool.astype('int').plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Precision for {'NN'} (TP / TP + FP): {sample[mask_true & (sample.good == True)].shape[0] / sample[mask_true].shape[0]}\")\n",
    "# print(f\"Recall for {'NN'} (TP / TP + FN): {sample[mask_true & (sample.good == True)].shape[0] / sample[sample.good == True].shape[0]}\")\n",
    "\n",
    "# # Precision for NN (TP / TP + FP): 0.6815068493150684\n",
    "# # Recall for NN (TP / TP + FN): 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = dd.read_parquet(\"../Census_samples/Whole_proc/Whole_1891_mom_pop_sp\").reset_index().compute(shcedule = client)\n",
    "right = dd.read_parquet(\"../Census_samples/Whole_proc/Whole_1881_mom_pop_sp\").reset_index().compute(schedule = client)\n",
    "report_ram_usage()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De-duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = sample.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 51 then 61\n",
    "# sample = sample.sort_values(['recid_2', 'nn'], ascending = [True, False]).drop_duplicates(subset = ['recid_2'], keep = 'first')\n",
    "# print(sample.nn_bool.value_counts())\n",
    "# sample = sample.sort_values(['recid_1', 'nn'], ascending = [True, False]).drop_duplicates(subset = ['recid_1'], keep = 'first')\n",
    "# print(sample.nn_bool.value_counts())\n",
    "\n",
    "# nn_bool\n",
    "# False    10303\n",
    "# True       241\n",
    "# Name: count, dtype: int64\n",
    "# nn_bool\n",
    "# True     206\n",
    "# False      5\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 61 then 51\n",
    "# matching_pool = matching_pool.sort_values(['recid_1', 'nn_pred'], ascending = [True, False]).drop_duplicates(subset = ['recid_1'], keep = 'first')\n",
    "# print(matching_pool.nn_bool.value_counts())\n",
    "# matching_pool = matching_pool.sort_values(['recid_2', 'nn_pred'], ascending = [True, False]).drop_duplicates(subset = ['recid_2'], keep = 'first')\n",
    "# print(matching_pool.nn_bool.value_counts())\n",
    "\n",
    "# assert matching_pool.recid_1.nunique() == matching_pool.shape[0]\n",
    "# assert matching_pool.recid_2.nunique() == matching_pool.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51 then 61\n",
    "def dedup(df, left_then_right = True):\n",
    "    if not left_then_right:\n",
    "        print(\"left_then_right = False\")\n",
    "        df = df.sort_values(['recid_2', 'nn_pred'], ascending = [True, False]).drop_duplicates(subset = ['recid_2'], keep = 'first')\n",
    "        # print(df.nn_bool.value_counts())\n",
    "        print(df.recid_1.nunique(), df.recid_2.nunique())\n",
    "        df = df.sort_values(['recid_1', 'nn_pred'], ascending = [True, False]).drop_duplicates(subset = ['recid_1'], keep = 'first')\n",
    "        # print(df.nn_bool.value_counts())\n",
    "        print(df.recid_1.nunique(), df.recid_2.nunique())\n",
    "        assert df.recid_1.nunique() == df.shape[0]\n",
    "        assert df.recid_2.nunique() == df.shape[0]\n",
    "    else:\n",
    "        print(\"left_then_right = True\")\n",
    "        df = df.sort_values(['recid_1', 'nn_pred'], ascending = [True, False]).drop_duplicates(subset = ['recid_1'], keep = 'first')\n",
    "        # print(df.nn_bool.value_counts())\n",
    "        print(df.recid_1.nunique(), df.recid_2.nunique())\n",
    "        df = df.sort_values(['recid_2', 'nn_pred'], ascending = [True, False]).drop_duplicates(subset = ['recid_2'], keep = 'first')\n",
    "        # print(df.nn_bool.value_counts())\n",
    "        print(df.recid_1.nunique(), df.recid_2.nunique())\n",
    "        assert df.recid_1.nunique() == df.shape[0]\n",
    "        assert df.recid_2.nunique() == df.shape[0]\n",
    "\n",
    "    return df.set_index(['recid_1', 'recid_2']).index\n",
    "\n",
    "# merge two deduped\n",
    "idx_91_81 = dedup(matching_pool, left_then_right = True)\n",
    "idx_81_91 = dedup(matching_pool, left_then_right = False)\n",
    "# unioni\n",
    "idx = idx_91_81.union(idx_81_91)\n",
    "del idx_91_81\n",
    "del idx_81_91\n",
    "# ffinal dedup\n",
    "idx = dedup(matching_pool.set_index(['recid_1', 'recid_2']).loc[idx][['nn_pred']].reset_index(), left_then_right = True)\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(f'../Output/{name}_final_idx', 'wb') as handle:\n",
    "    pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = ['uk1891a_age', 'pname_61', 'pname_51', 'sname_61', 'sname_51',\\\n",
    "#                     'pname_soundex_61', 'pname_soundex_51', 'sname_soundex_61', 'sname_soundex_51',\\\n",
    "#                     'pname_metaphone_61', 'pname_metaphone_51', 'sname_metaphone_61', 'sname_metaphone_51',\\\n",
    "#                     'pname_pop_soundex_61', 'pname_pop_soundex_51', 'sname_pop_soundex_61', 'sname_pop_soundex_51',\\\n",
    "#                     'pname_mom_soundex_61', 'pname_mom_soundex_51', 'sname_mom_soundex_61', 'sname_mom_soundex_51',\\\n",
    "#                     'pname_sp_soundex_61', 'pname_sp_soundex_51', 'sname_sp_soundex_61', 'sname_sp_soundex_51',\\\n",
    "#                     ]  \n",
    "\n",
    "# feature_names get rid of suffix\n",
    "feature_names = [ 'pname', 'oname', 'sname',\\\n",
    "                    'pname_soundex', 'sname_soundex',\\\n",
    "                    'pname_metaphone', 'sname_metaphone',\\\n",
    "                    'pname_pop_soundex', 'sname_pop_soundex',\\\n",
    "                    'pname_mom_soundex', 'sname_mom_soundex',\\\n",
    "                    'pname_sp_soundex', 'sname_sp_soundex',\\\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_81_61 = False\n",
    "MINIMUM_AGE = 18 if is_81_61 else 8\n",
    "\n",
    "def attach_ori_feature(to_attach, to_attach_recid_left, to_attach_recid_right, left_df, right_df, suffixes, exclude_lbirthy = False):\n",
    "    to_attach = to_attach.merge(left_df, left_on = to_attach_recid_left, right_on = 'recid', how = 'left')\n",
    "\n",
    "    if exclude_lbirthy:\n",
    "        age_mask = (to_attach.uk1891a_age >= MINIMUM_AGE)\n",
    "        to_attach = to_attach[age_mask]\n",
    "\n",
    "    to_attach = to_attach.merge(right_df, left_on = to_attach_recid_right, right_on = 'recid', how = 'left', suffixes = suffixes)\n",
    "    \n",
    "    return to_attach\n",
    "\n",
    "# attached = attach_ori_feature(matching_pool[['recid_1', 'recid_2', 'nn_bool', 'nn_pred']], 'recid_1', 'recid_2', left[['recid'] + feature_names + ['uk1891a_age']], right[['recid'] + feature_names], ['_81', '_61'])\n",
    "attached_filtered_lbirthy = attach_ori_feature(idx.to_frame().reset_index(drop = True), 'recid_1', 'recid_2', left[['recid'] + feature_names + ['uk1891a_age']], right[['recid'] + feature_names], ['_91', '_81'], exclude_lbirthy = True)\n",
    "attached_filtered_lbirthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'../Output/{name}_final_idx_lbirthy_filtered', 'wb') as handle:\n",
    "    to_sav = attached_filtered_lbirthy.set_index(['recid_1', 'recid_2']).index\n",
    "    pickle.dump(to_sav, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"saved lbirthy_filtered, {to_sav.get_level_values(0).nunique()}, {to_sav.get_level_values(1).nunique()} pairs\")\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in attached_filtered_lbirthy.columns if 'age' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask_validation(pred_look, pred, recid_left = 'recid_1', recid_right = 'recid_2', metrics = False, whole_mt_pool = False, only_true = False,  year_suffix='91-81', left_age_col = 'uk1891a_age', exclude_lbirthy = False):\n",
    "    \"\"\"\n",
    "    This code block defines several masks for filtering data based on certain conditions.\n",
    "    The masks are used to select specific rows from the dataset.\n",
    "    - `mask_matches_not_born_in_smaller_year`: A mask that filters rows where the age in 1881 is less than 10.\n",
    "    - `mask_pop_name`: A mask that filters rows where the population soundex names in 1881 and 1891 match, or if either of them is missing.\n",
    "    - `mask_mom_name`: A mask that filters rows where the mother's soundex names in 1881 and 1891 match, or if either of them is missing.\n",
    "    - `mask_spoouse_name`: A mask that filters rows where the spouse's soundex names in 1881 and 1891 match, or if either of them is missing.\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "    # masks\n",
    "    mask_pred_Positive = pred_look[pred] == True\n",
    "    year_old, year_new = year_suffix.split('-')\n",
    "    if only_true == False:\n",
    "        # back_up = pred_look.copy()\n",
    "        pred_look = pred_look[mask_pred_Positive]\n",
    "\n",
    "    def define_masks(df):\n",
    "        # by excluding those not born in smaller year, we neglected immigration and adoption\n",
    "        AGE_LIMIT = 18 if left_age_col == 'uk1881a_age' else 8\n",
    "        mask_matches_born_in_smaller_year = df[left_age_col] >= AGE_LIMIT\n",
    "        # mask_name = df.pname_61 == df.pname_51\n",
    "        \n",
    "        mask_sname_literal = (df[f'sname_{year_old}'] == df[f'sname_{year_new}']) | (df[f'sname_{year_old}'].isna() | df[f'sname_{year_new}'].isna())\n",
    "        mask_sname_soundex = (df[f'sname_soundex_{year_old}'] == df[f'sname_soundex_{year_new}']) | (df[f'sname_soundex_{year_old}'].isna() | df[f'sname_soundex_{year_new}'].isna())\n",
    "        mask_sname_metaphone = (df[f'sname_metaphone_{year_old}'] == df[f'sname_metaphone_{year_new}']) | (df[f'sname_metaphone_{year_old}'].isna() | df[f'sname_metaphone_{year_new}'].isna())\n",
    "\n",
    "        mask_pname_literal = (df[f'pname_{year_old}'] == df[f'pname_{year_new}']) | (df[f'pname_{year_old}'].isna() | df[f'pname_{year_new}'].isna())\n",
    "        mask_pname_soundex = (df[f'pname_soundex_{year_old}'] == df[f'pname_soundex_{year_new}']) | (df[f'pname_soundex_{year_old}'].isna() | df[f'pname_soundex_{year_new}'].isna())\n",
    "        mask_pname_metaphone = (df[f'pname_metaphone_{year_old}'] == df[f'pname_metaphone_{year_new}']) | (df[f'pname_metaphone_{year_old}'].isna() | df[f'pname_metaphone_{year_new}'].isna())\n",
    "\n",
    "        # fam \n",
    "        mask_pop_name = (df[f'pname_pop_soundex_{year_old}'] == df[f'pname_pop_soundex_{year_new}']) | (df[f'pname_pop_soundex_{year_old}'].isna() | df[f'pname_pop_soundex_{year_new}'].isna())\n",
    "        mask_mom_name = (df[f'pname_mom_soundex_{year_old}'] == df[f'pname_mom_soundex_{year_new}']) | (df[f'pname_mom_soundex_{year_old}'].isna() | df[f'pname_mom_soundex_{year_new}'].isna())\n",
    "        mask_sp_name = (df[f'pname_sp_soundex_{year_old}'] == df[f'pname_sp_soundex_{year_new}']) | (df[f'pname_sp_soundex_{year_old}'].isna() | df[f'pname_sp_soundex_{year_new}'].isna())\n",
    "\n",
    "        # make all value_counts of masks into a dataframe\n",
    "        st = pd.DataFrame([ mask_sname_literal.value_counts().rename('sname_literal'), \\\n",
    "                            mask_sname_soundex.value_counts().rename('sname_soundex'), \\\n",
    "                            mask_sname_metaphone.value_counts().rename('sname_metaphone'), \\\n",
    "                            mask_pname_literal.value_counts().rename('pname_literal'), \\\n",
    "                            mask_pname_soundex.value_counts().rename('pname_soundex'), \\\n",
    "                            mask_pname_metaphone.value_counts().rename('pname_metaphone'), \\\n",
    "                            mask_matches_born_in_smaller_year.value_counts().rename('matches_born_in_smaller_year'), \\\n",
    "                            mask_pop_name.value_counts().rename('pop_name'), \\\n",
    "                            mask_mom_name.value_counts().rename('mom_name'), \\\n",
    "                            mask_sp_name.value_counts().rename('sp_name')])\n",
    "        # return all the masks\n",
    "        return st, mask_sname_literal, mask_sname_soundex, mask_sname_metaphone, \\\n",
    "                mask_pname_literal, mask_pname_soundex, mask_pname_metaphone, \\\n",
    "                mask_matches_born_in_smaller_year, mask_pop_name, mask_mom_name, mask_sp_name, AGE_LIMIT\n",
    "    \n",
    "    # run the function\n",
    "    st, mask_sname_literal, mask_sname_soundex, mask_sname_metaphone, \\\n",
    "    mask_pname_literal, mask_pname_soundex, mask_pname_metaphone, \\\n",
    "    mask_matches_born_in_smaller_year, mask_pop_name, mask_mom_name, mask_sp_name, AGE_LIMIT = define_masks(pred_look)\n",
    "\n",
    "    # format cell numiber to human readable\n",
    "    st.style.format(\"{:,}\")\n",
    "    # make a visualization of percentage of each mask\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # translate all numbers to perventage to pred_look[0], pay attention to NaN\n",
    "    st.T.div(st.T.sum(axis=1), axis=0).mul(100).plot.bar(ax=ax)\n",
    "    st.T.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Mask value counts\")\n",
    "    ax.set_xlabel(\"Mask name\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    def metrics(look, whole_mt_pool = False):\n",
    "        '''\n",
    "        calculate optimistic precision, precision, recall, f1 score\n",
    "        '''\n",
    "        st, mask_sname_literal, mask_sname_soundex, mask_sname_metaphone, \\\n",
    "        mask_pname_literal, mask_pname_soundex, mask_pname_metaphone, \\\n",
    "        mask_matches_born_in_smaller_year, mask_pop_name, mask_mom_name, mask_sp_name, AGE_LIMIT = define_masks(pred_look)\n",
    "        print(mask_pop_name | mask_mom_name | mask_sp_name)\n",
    "        mask_TP_optimistic = (mask_pop_name | mask_mom_name | mask_sp_name) & (mask_sname_metaphone | mask_pname_metaphone)\n",
    "        mask_TP = ((mask_pop_name & mask_mom_name) | mask_sp_name) & (mask_sname_metaphone | mask_pname_metaphone) & mask_matches_born_in_smaller_year\n",
    "        # optimistic precision\n",
    "        optimistic_precision = look[mask_TP_optimistic].shape[0] / look[mask_pred_Positive].shape[0]\n",
    "        # precision\n",
    "        precision_safe_ground = look[mask_TP].shape[0] / look[mask_pred_Positive].shape[0]\n",
    "        # recall\n",
    "        if whole_mt_pool:\n",
    "            matching_rate_1881 = look[look[pred] == True][recid_left].nunique() / right.recid.nunique() \n",
    "            if exclude_lbirthy:\n",
    "                matching_rate_1891 = look[look[pred] == True][recid_right].nunique() / left[left[left_age_col] > AGE_LIMIT].recid.nunique()\n",
    "            else:\n",
    "                matching_rate_1891 = look[look[pred] == True][recid_right].nunique() / left.recid.nunique()\n",
    "            \n",
    "        # else:\n",
    "        #     intermediate = filtered_pool.reset_index()\n",
    "        #     matching_rate_1881 = look[look[pred] == True][recid_left].nunique() / intermediate.recid_1.nunique() \n",
    "        #     matching_rate_1891 = look[look[pred] == True][recid_right].nunique() / intermediate.recid_2.nunique()\n",
    "        # return a tuple\n",
    "        # return (optimistic_precision, precision, matching_rate_1881, matching_rate_1891)\n",
    "\n",
    "        # format to 2 decimal places\n",
    "        stats = pd.DataFrame({'optimistic_precision': [optimistic_precision], \\\n",
    "                             'precision': [precision_safe_ground], \\\n",
    "                             'matching_rate_1881': [matching_rate_1881], \\\n",
    "                             'matching_rate_1891': [matching_rate_1891]\n",
    "                            }\n",
    "                            )\n",
    "        stats.style.format(\"{:.2f}\", na_rep=\"-\", subset=['optimistic_precision', 'precision', \\\n",
    "                                                         'matching_rate_1881', 'matching_rate_1891'\n",
    "                                                         ]\n",
    "                                                         )\n",
    "        return stats\n",
    "    \n",
    "    if metrics:\n",
    "        return st.T, metrics(pred_look, whole_mt_pool = whole_mt_pool)\n",
    "    else:\n",
    "        return st.T\n",
    "\n",
    "attached_filtered_lbirthy['nn_bool'] = True\n",
    "mask_validation(attached_filtered_lbirthy, pred = 'nn_bool', whole_mt_pool = True, metrics = True, exclude_lbirthy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "# with open(f'../Output/idx_91-81_0_81_91.sav', 'wb') as handle:\n",
    "#     idx = attached.drop_duplicates(subset = ['recid_1', 'recid_2'], keep = 'first').set_index(['recid_1', 'recid_2']).index\n",
    "#     # make idx unique, maybe there are cases like (1, 2), (2, 1)\n",
    "#     # idx = pd.MultiIndex.from_tuples([(i, j) if i < j else (j, i) for i, j in idx])\n",
    "#     assert idx.is_unique\n",
    "#     pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Excluding lbirthyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attached = attach_ori_feature(idx.to_frame().reset_index(drop = True), 'recid_1', 'recid_2', left[['recid'] + feature_names + ['uk1891a_age']], right[['recid'] + feature_names], ['_91', '_81'])\n",
    "attached['nn_bool'] = True\n",
    "mask_validation(attached, pred = 'nn_bool', whole_mt_pool = True, metrics = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
